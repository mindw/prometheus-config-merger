groups:
- name: cloudwatch_ASG1
  rules:
  - alert: ASGFailToScale
    annotations:
      description: ASG {{ $labels.auto_scaling_group_name }} attempts to scale up
        to {{ $value }} instances for past 10 minutes
      summary: Autoscaling group fails to scale up
    expr: max by (auto_scaling_group_name)(aws_autoscaling_group_desired_capacity_maximum
      offset 300s) > max by (auto_scaling_group_name)(aws_autoscaling_group_in_service_instances_maximum
      offset 300s)
    for: 10m
    labels:
      service: ASG
      severity: warning
  - alert: ASGUnevenAcrossAZs
    annotations:
      description: '"Number of instances in the ASG {{ $labels.auto_scaling_group_name
        }} can''t be evenly distributed across the defined number of availability
        zones"'
      summary: Autoscaling group can't be balanced across AZs
    expr: max(aws_autoscaling_group_in_service_instances_maximum offset 300s) by (auto_scaling_group_name)
      % scalar(count(count(kube_node_labels) by (label_topology_kubernetes_io_zone)))
      > 0
    for: 10m
    labels:
      service: ASG
      severity: warning
  - alert: ASGInstancesFailToJoin
    annotations:
      description: '"Number of active instances in the ASG {{ $labels.auto_scaling_group_name
        }} is greater than number of the k8s cluster nodes of the type {{ $labels.label_NodeType
        }}"'
      summary: Members of an autoscaling group are failing to join the k8s cluster
    expr: label_replace(aws_autoscaling_group_in_service_instances_maximum offset
      5m, "label_NodeType", "$1", "auto_scaling_group_name", ".*-([^-]+)$") > on (label_NodeType)
      group_right(auto_scaling_group_name) count(kube_node_labels offset 300s) by
      (label_NodeType)
    for: 10m
    labels:
      service: ASG
      severity: warning
- name: cloudwatch_LB
  rules:
  - alert: ALBTargetGroupUnhealthy
    annotations:
      description: No healthy targets {{ $value }} in the target group {{ $labels.target_group
        }} of the AWS application load balancer {{ $labels.load_balancer }}"
      summary: No healthy hosts in one of the ALB's target groups
    expr: max(aws_applicationelb_healthy_host_count_maximum) by (target_group, load_balancer)
      == 0
    for: 5m
    labels:
      service: ALB
      severity: warning
  - alert: ALBHighTargetConnectionFailureRate
    annotations:
      description: '"There were {{ $value }} failed connection attempts from AWS application
        load balancer {{ $labels.load_balancer }} to targets in target group {{ $labels.target_group
        }} during last 5 minutes"'
      summary: Problems during establishing connections from an ALB instance to one
        of its targets
    expr: max(aws_applicationelb_target_connection_error_count_sum) by (target_group,
      load_balancer) > 0
    for: 5m
    labels:
      service: ALB
      severity: warning
  - alert: ALBHighNumberOf5XX
    annotations:
      description: '"There were {{ $value }} 5XX server error codes originating from
        the load balancer {{ $labels.load_balancer }} during last 5 minutes"'
      summary: High rate of 5XX error codes originating from an ALB instance
    expr: max(aws_applicationelb_http_code_elb_5_xx_count_sum offset 300s) by (load_balancer)
      > 0
    for: 5m
    labels:
      service: ALB
      severity: warning
  - alert: ALBMaxConnectionsLimitReached
    annotations:
      description: '"There were {{ $value }} connections rejected because the load
        balancer {{ $labels.load_balancer }} had reached its maximum number of connections
        during last 5 minutes"'
      summary: Maximum number of connections is reached by an ALB instance
    expr: max(aws_applicationelb_rejected_connection_count_sum offset 300s) by (load_balancer)
      > 0
    for: 5m
    labels:
      service: ALB
      severity: warning
- concurrency: 10
  interval: 30s
  name: node-exporter.rules
  rules:
  - expr: |
      count without (cpu, mode) (
        node_cpu_seconds_total{job="node-exporter",mode="idle"}
      )
    record: instance:node_num_cpu:sum
  - expr: |
      1 - avg without (cpu) (
        sum without (mode) (rate(node_cpu_seconds_total{job="node-exporter", mode=~"idle|iowait|steal"}[5m]))
      )
    record: instance:node_cpu_utilisation:rate5m
  - expr: |
      (
        node_load1{job="node-exporter"}
      /
        instance:node_num_cpu:sum{job="node-exporter"}
      )
    record: instance:node_load1_per_cpu:ratio
  - expr: |
      1 - (
        (
          node_memory_MemAvailable_bytes{job="node-exporter"}
          or
          (
            node_memory_Buffers_bytes{job="node-exporter"}
            +
            node_memory_Cached_bytes{job="node-exporter"}
            +
            node_memory_MemFree_bytes{job="node-exporter"}
            +
            node_memory_Slab_bytes{job="node-exporter"}
          )
        )
      /
        node_memory_MemTotal_bytes{job="node-exporter"}
      )
    record: instance:node_memory_utilisation:ratio
  - expr: |
      rate(node_vmstat_pgmajfault{job="node-exporter"}[5m])
    record: instance:node_vmstat_pgmajfault:rate5m
  - expr: |
      rate(node_disk_io_time_seconds_total{job="node-exporter", device!=""}[5m])
    record: instance_device:node_disk_io_time_seconds:rate5m
  - expr: |
      rate(node_disk_io_time_weighted_seconds_total{job="node-exporter", device!=""}[5m])
    record: instance_device:node_disk_io_time_weighted_seconds:rate5m
  - expr: |
      sum without (device) (
        rate(node_network_receive_bytes_total{job="node-exporter", device!="lo"}[5m])
      )
    record: instance:node_network_receive_bytes_excluding_lo:rate5m
  - expr: |
      sum without (device) (
        rate(node_network_transmit_bytes_total{job="node-exporter", device!="lo"}[5m])
      )
    record: instance:node_network_transmit_bytes_excluding_lo:rate5m
  - expr: |
      sum without (device) (
        rate(node_network_receive_drop_total{job="node-exporter", device!="lo"}[5m])
      )
    record: instance:node_network_receive_drop_excluding_lo:rate5m
  - expr: |
      sum without (device) (
        rate(node_network_transmit_drop_total{job="node-exporter", device!="lo"}[5m])
      )
    record: instance:node_network_transmit_drop_excluding_lo:rate5m
- concurrency: 10
  interval: 60s
  name: node-exporter.alerts
  rules:
  - alert: NodeFilesystemSpaceFillingUp
    annotations:
      description: Filesystem on {{ $labels.device }}, mounted on {{ $labels.mountpoint
        }}, at {{ $labels.node }} has only {{ printf "%.2f" $value }}% available space
        left and is filling up.
      summary: Filesystem is predicted to run out of space within the next 24 hours.
    expr: |
      (
        node_filesystem_avail_bytes{job="node-exporter",fstype!~"(tmpfs|ramfs|)",mountpoint!~"/(run/containerd/.+|var/lib/kubelet/pods/.+)"} / node_filesystem_size_bytes{job="node-exporter",fstype!~"(tmpfs|ramfs|)",mountpoint!~"/(run/containerd/.+|var/lib/kubelet/pods/.+)"} * 100 < 40
      and
        predict_linear(node_filesystem_avail_bytes{job="node-exporter",fstype!~"(tmpfs|ramfs|)",mountpoint!~"/(run/containerd/.+|var/lib/kubelet/pods/.+)"}[6h], 24*60*60) < 0
      and
        node_filesystem_readonly{job="node-exporter",fstype!~"(tmpfs|ramfs|)",mountpoint!~"/(run/containerd/.+|var/lib/kubelet/pods/.+)"} == 0
      )
    for: 1h
    labels:
      service: node
      severity: warning
  - alert: NodeFilesystemSpaceFillingUp
    annotations:
      description: Filesystem on {{ $labels.device }}, mounted on {{ $labels.mountpoint
        }}, at {{ $labels.node }} has only {{ printf "%.2f" $value }}% available space
        left and is filling up fast.
      summary: Filesystem is predicted to run out of space within the next 4 hours.
    expr: |
      (
        node_filesystem_avail_bytes{job="node-exporter",fstype!~"(tmpfs|ramfs|)",mountpoint!~"/(run/containerd/.+|var/lib/kubelet/pods/.+)"} / node_filesystem_size_bytes{job="node-exporter",fstype!~"(tmpfs|ramfs|)",mountpoint!~"/(run/containerd/.+|var/lib/kubelet/pods/.+)"} * 100 < 20
      and
        predict_linear(node_filesystem_avail_bytes{job="node-exporter",fstype!~"(tmpfs|ramfs|)",mountpoint!~"/(run/containerd/.+|var/lib/kubelet/pods/.+)"}[6h], 4*60*60) < 0
      and
        node_filesystem_readonly{job="node-exporter",fstype!~"(tmpfs|ramfs|)",mountpoint!~"/(run/containerd/.+|var/lib/kubelet/pods/.+)"} == 0
      )
    for: 1h
    labels:
      service: node
      severity: critical
  - alert: NodeFilesystemAlmostOutOfSpace
    annotations:
      description: Filesystem on {{ $labels.device }}, mounted on {{ $labels.mountpoint
        }}, at {{ $labels.node }} has only {{ printf "%.2f" $value }}% available space
        left.
      summary: Filesystem has less than 5% space left.
    expr: |
      (
        node_filesystem_avail_bytes{job="node-exporter",fstype!~"(tmpfs|ramfs|)",mountpoint!~"/(run/containerd/.+|var/lib/kubelet/pods/.+)"} / node_filesystem_size_bytes{job="node-exporter",fstype!~"(tmpfs|ramfs|)",mountpoint!~"/(run/containerd/.+|var/lib/kubelet/pods/.+)"} * 100 < 5
      and
        node_filesystem_readonly{job="node-exporter",fstype!~"(tmpfs|ramfs|)",mountpoint!~"/(run/containerd/.+|var/lib/kubelet/pods/.+)"} == 0
      )
    for: 30m
    labels:
      service: node
      severity: warning
  - alert: NodeFilesystemAlmostOutOfSpace
    annotations:
      description: Filesystem on {{ $labels.device }}, mounted on {{ $labels.mountpoint
        }}, at {{ $labels.node }} has only {{ printf "%.2f" $value }}% available space
        left.
      summary: Filesystem has less than 3% space left.
    expr: |
      (
        node_filesystem_avail_bytes{job="node-exporter",fstype!~"(tmpfs|ramfs|)",mountpoint!~"/(run/containerd/.+|var/lib/kubelet/pods/.+)"} / node_filesystem_size_bytes{job="node-exporter",fstype!~"(tmpfs|ramfs|)",mountpoint!~"/(run/containerd/.+|var/lib/kubelet/pods/.+)"} * 100 < 3
      and
        node_filesystem_readonly{job="node-exporter",fstype!~"(tmpfs|ramfs|)",mountpoint!~"/(run/containerd/.+|var/lib/kubelet/pods/.+)"} == 0
      )
    for: 30m
    labels:
      service: node
      severity: critical
  - alert: NodeFilesystemFilesFillingUp
    annotations:
      description: Filesystem on {{ $labels.device }}, mounted on {{ $labels.mountpoint
        }}, at {{ $labels.node }} has only {{ printf "%.2f" $value }}% available inodes
        left and is filling up.
      summary: Filesystem is predicted to run out of inodes within the next 24 hours.
    expr: |
      (
        node_filesystem_files_free{job="node-exporter",fstype!~"(tmpfs|ramfs|)",mountpoint!~"/(run/containerd/.+|var/lib/kubelet/pods/.+)"} / node_filesystem_files{job="node-exporter",fstype!~"(tmpfs|ramfs|)",mountpoint!~"/(run/containerd/.+|var/lib/kubelet/pods/.+)"} * 100 < 40
      and
        predict_linear(node_filesystem_files_free{job="node-exporter",fstype!~"(tmpfs|ramfs|)",mountpoint!~"/(run/containerd/.+|var/lib/kubelet/pods/.+)"}[6h], 24*60*60) < 0
      and
        node_filesystem_readonly{job="node-exporter",fstype!~"(tmpfs|ramfs|)",mountpoint!~"/(run/containerd/.+|var/lib/kubelet/pods/.+)"} == 0
      )
    for: 1h
    labels:
      service: node
      severity: warning
  - alert: NodeFilesystemFilesFillingUp
    annotations:
      description: Filesystem on {{ $labels.device }}, mounted on {{ $labels.mountpoint
        }}, at {{ $labels.node }} has only {{ printf "%.2f" $value }}% available inodes
        left and is filling up fast.
      summary: Filesystem is predicted to run out of inodes within the next 4 hours.
    expr: |
      (
        node_filesystem_files_free{job="node-exporter",fstype!~"(tmpfs|ramfs|)",mountpoint!~"/(run/containerd/.+|var/lib/kubelet/pods/.+)"} / node_filesystem_files{job="node-exporter",fstype!~"(tmpfs|ramfs|)",mountpoint!~"/(run/containerd/.+|var/lib/kubelet/pods/.+)"} * 100 < 20
      and
        predict_linear(node_filesystem_files_free{job="node-exporter",fstype!~"(tmpfs|ramfs|)",mountpoint!~"/(run/containerd/.+|var/lib/kubelet/pods/.+)"}[6h], 4*60*60) < 0
      and
        node_filesystem_readonly{job="node-exporter",fstype!~"(tmpfs|ramfs|)",mountpoint!~"/(run/containerd/.+|var/lib/kubelet/pods/.+)"} == 0
      )
    for: 1h
    labels:
      service: node
      severity: critical
  - alert: NodeFilesystemAlmostOutOfFiles
    annotations:
      description: Filesystem on {{ $labels.device }}, mounted on {{ $labels.mountpoint
        }}, at {{ $labels.instance }} has only {{ printf "%.2f" $value }}% available
        inodes left.
      summary: Filesystem has less than 5% inodes left.
    expr: |
      (
        node_filesystem_files_free{job="node-exporter",fstype!~"(tmpfs|ramfs|)",mountpoint!~"/(run/containerd/.+|var/lib/kubelet/pods/.+)"} / node_filesystem_files{job="node-exporter",fstype!~"(tmpfs|ramfs|)",mountpoint!~"/(run/containerd/.+|var/lib/kubelet/pods/.+)"} * 100 < 5
      and
        node_filesystem_readonly{job="node-exporter",fstype!~"(tmpfs|ramfs|)",mountpoint!~"/(run/containerd/.+|var/lib/kubelet/pods/.+)"} == 0
      )
    for: 1h
    labels:
      service: node
      severity: warning
  - alert: NodeFilesystemAlmostOutOfFiles
    annotations:
      description: Filesystem on {{ $labels.device }}, mounted on {{ $labels.mountpoint
        }}, at {{ $labels.instance }} has only {{ printf "%.2f" $value }}% available
        inodes left.
      summary: Filesystem has less than 3% inodes left.
    expr: |
      (
        node_filesystem_files_free{job="node-exporter",fstype!~"(tmpfs|ramfs|)",mountpoint!~"/(run/containerd/.+|var/lib/kubelet/pods/.+)"} / node_filesystem_files{job="node-exporter",fstype!~"(tmpfs|ramfs|)",mountpoint!~"/(run/containerd/.+|var/lib/kubelet/pods/.+)"} * 100 < 3
      and
        node_filesystem_readonly{job="node-exporter",fstype!~"(tmpfs|ramfs|)",mountpoint!~"/(run/containerd/.+|var/lib/kubelet/pods/.+)"} == 0
      )
    for: 1h
    labels:
      service: node
      severity: critical
  - alert: NodeNetworkReceiveErrs
    annotations:
      description: '{{ $labels.instance }} interface {{ $labels.device }} has encountered
        {{ printf "%.0f" $value }} receive errors in the last two minutes.'
      summary: Network interface is reporting many receive errors.
    expr: |
      rate(node_network_receive_errs_total{job="node-exporter"}[2m]) / rate(node_network_receive_packets_total{job="node-exporter"}[2m]) > 0.01
    for: 1h
    labels:
      service: node
      severity: warning
  - alert: NodeNetworkTransmitErrs
    annotations:
      description: '{{ $labels.instance }} interface {{ $labels.device }} has encountered
        {{ printf "%.0f" $value }} transmit errors in the last two minutes.'
      summary: Network interface is reporting many transmit errors.
    expr: |
      rate(node_network_transmit_errs_total{job="node-exporter"}[2m]) / rate(node_network_transmit_packets_total{job="node-exporter"}[2m]) > 0.01
    for: 1h
    labels:
      service: node
      severity: warning
  - alert: NodeHighNumberConntrackEntriesUsed
    annotations:
      description: '{{ $value | humanizePercentage }} of conntrack entries are used.'
      summary: Number of conntrack are getting close to the limit.
    expr: |
      (node_nf_conntrack_entries{job="node-exporter"} / node_nf_conntrack_entries_limit) > 0.75
    labels:
      service: node
      severity: warning
  - alert: NodeTextFileCollectorScrapeError
    annotations:
      description: Node Exporter text file collector on {{ $labels.instance }} failed
        to scrape.
      summary: Node Exporter text file collector failed to scrape.
    expr: |
      node_textfile_scrape_error{job="node-exporter"} == 1
    labels:
      service: node
      severity: warning
  - alert: NodeClockSkewDetected
    annotations:
      description: Clock at {{ $labels.instance }} is out of sync by more than 0.05s.
        Ensure NTP is configured correctly on this host.
      summary: Clock skew detected.
    expr: |
      (
        node_timex_offset_seconds{job="node-exporter"} > 0.05
      and
        deriv(node_timex_offset_seconds{job="node-exporter"}[5m]) >= 0
      )
      or
      (
        node_timex_offset_seconds{job="node-exporter"} < -0.05
      and
        deriv(node_timex_offset_seconds{job="node-exporter"}[5m]) <= 0
      )
    for: 10m
    labels:
      service: node
      severity: warning
  - alert: NodeClockNotSynchronising
    annotations:
      description: Clock at {{ $labels.instance }} is not synchronising. Ensure NTP
        is configured on this host.
      summary: Clock not synchronising.
    expr: |
      min_over_time(node_timex_sync_status{job="node-exporter"}[5m]) == 0
      and
      node_timex_maxerror_seconds{job="node-exporter"} >= 16
    for: 10m
    labels:
      service: node
      severity: warning
  - alert: NodeFileDescriptorLimit
    annotations:
      description: File descriptors limit at {{ $labels.instance }} is currently at
        {{ printf "%.2f" $value }}%.
      summary: Kernel is predicted to exhaust file descriptors limit soon.
    expr: |
      (
        node_filefd_allocated{job="node-exporter"} * 100 / node_filefd_maximum{job="node-exporter"} > 70
      )
    for: 15m
    labels:
      service: node
      severity: warning
  - alert: NodeFileDescriptorLimit
    annotations:
      description: File descriptors limit at {{ $labels.instance }} is currently at
        {{ printf "%.2f" $value }}%.
      summary: Kernel is predicted to exhaust file descriptors limit soon.
    expr: |
      (
        node_filefd_allocated{job="node-exporter"} * 100 / node_filefd_maximum{job="node-exporter"} > 90
      )
    for: 15m
    labels:
      service: node
      severity: critical
  - alert: NodeCPUHighUsage
    annotations:
      description: |
        CPU usage at {{ $labels.instance }} has been above 85% for the last 15 minutes, is currently at {{ printf "%.2f" $value }}%.
      summary: High CPU usage.
    expr: |
      sum without(mode) (avg without (cpu) (rate(node_cpu_seconds_total{job="node-exporter", mode!="idle"}[2m]))) * 100 > 85
    for: 15m
    labels:
      service: node
      severity: info
  - alert: NodeSystemSaturation
    annotations:
      description: |
        System load per core at {{ $labels.instance }} has been above 2 for the last 15 minutes, is currently at {{ printf "%.2f" $value }}.
        This might indicate this instance resources saturation and can cause it becoming unresponsive.
      summary: System saturated, load per core is very high.
    expr: |
      node_load1{job="node-exporter"}
      / count without (cpu, mode) (node_cpu_seconds_total{job="node-exporter", mode="idle"}) > 2
    for: 15m
    labels:
      service: node
      severity: warning
  - alert: NodeMemoryMajorPagesFaults
    annotations:
      description: |
        Memory major pages are occurring at very high rate at {{ $labels.node }}, 500 major page faults per second for the last 15 minutes, is currently at {{ printf "%.2f" $value }}.
        Please check that there is enough memory available at this instance.
      summary: Memory major page faults are occurring at very high rate.
    expr: |
      rate(node_vmstat_pgmajfault{job="node-exporter"}[5m]) > 500
    for: 15m
    labels:
      service: node
      severity: warning
  - alert: NodeMemoryHighUtilization
    annotations:
      description: |
        Memory is filling up at {{ $labels.node }}, has been above 90% for the last 15 minutes, is currently at {{ printf "%.2f" $value }}%.
      summary: Host is running out of memory.
    expr: |
      100 - (node_memory_MemAvailable_bytes{job="node-exporter"} / node_memory_MemTotal_bytes{job="node-exporter"} * 100) > 90
    for: 15m
    labels:
      service: node
      severity: warning
  - alert: NodeDiskIOSaturation
    annotations:
      description: |
        Disk IO queue (aqu-sq) is high on {{ $labels.device }} at {{ $labels.node }}, has been above 10 for the last 15 minutes, is currently at {{ printf "%.2f" $value }}.
        This symptom might indicate disk saturation.
      summary: Disk IO queue is high.
    expr: |
      rate(node_disk_io_time_weighted_seconds_total{job="node-exporter", device!=""}[5m]) > 10
    for: 30m
    labels:
      severity: warning
- concurrency: 10
  interval: 60s
  name: node-exporter-ec2-network.alerts
  rules:
  - alert: AWSEC2PPSAllowanceExceeded
    annotations:
      description: |
        Packets are dropped/queued because the bidirectional PPS exceeds the maximum for more then 2 minutes -
        for device {{ $labels.device }}' at {{ $labels.instance }}.
        PPS allowance is enforced separately to the overall bandwidth allowance and, while the instance may
        still be under overall bandwidth allowance, the PPS allowance may exceed if the mean packet size is small.
      summary: '{{ $labels.engageli_stack_name }}/{{ $labels.engageli_role }}: AWS
        Node bidirectional PPS allowance is exceeded'
    expr: |
      label_move(
        rate(node_ethtool_pps_allowance_exceeded{}[2m]) > 0,
        "engageli_instance", "instance"
      )
    for: 2m
    labels:
      service: node-ec2
      severity: critical
  - alert: AWSEC2BWINAllowanceExceeded
    annotations:
      description: |
        Packets are dropped/queued because the inbound bandwidth allowance was exceeded for more then 2 minutes -
        for device {{ $labels.device }}' at {{ $labels.instance }}.
      summary: '{{ $labels.engageli_stack_name }}/{{ $labels.engageli_role }}: AWS
        Node inbound aggregate bandwidth allowance is exceeded'
    expr: |
      label_move(
        rate(node_ethtool_bw_in_allowance_exceeded{}[2m]) > 0,
        "engageli_instance", "instance"
      )
    for: 2m
    labels:
      service: node-ec2
      severity: critical
  - alert: AWSEC2BWOutAllowanceExceeded
    annotations:
      description: |
        Packets are dropped/queued because the outbound bandwidth allowance was exceeded for more then 2 minutes -
        for device {{ $labels.device }}' at {{ $labels.instance }}.
      summary: '{{ $labels.engageli_stack_name }}/{{ $labels.engageli_role }}: AWS
        Node outbound aggregate bandwidth allowance is exceeded'
    expr: |
      label_move(
        rate(node_ethtool_bw_out_allowance_exceeded{}[2m]) > 0,
        "engageli_instance", "instance"
      )
    for: 2m
    labels:
      service: node-ec2
      severity: critical
  - alert: AWSEC2ConntrackAllowanceExceeded
    annotations:
      description: |
        Packets are dropped/queued because due to exhaustion of tracked session allowance for more then 2 minutes -
        for device {{ $labels.device }}' at {{ $labels.instance }}.
        New sessions fail to establish once this allowance is exceeded. Sessions are restored
        once instance session count drops below the allowance.
      summary: '{{ $labels.engageli_stack_name }}/{{ $labels.engageli_role }}: AWS
        Node tracked session allowance is exhausted.'
    expr: |
      label_move(
        rate(node_ethtool_conntrack_allowance_exceeded{}[2m]) > 0,
        "engageli_instance", "instance"
      )
    for: 2m
    labels:
      service: node-ec2
      severity: critical
  - alert: AWSEC2LinkLocalAllowanceExceeded
    annotations:
      description: |
        Packets are dropped/queued due to PPS rate allowance exceeded for local services for more then 2 minutes -
        for device {{ $labels.device }}' at {{ $labels.instance }}.
        Local services are Route 53 DNS Resolver, Instance Metadata Service, Amazon Time Sync Service and the like.
        This often points to suboptimal design choices or misconfiguration. This allowance is the same across instances.
      summary: '{{ $labels.engageli_stack_name }}/{{ $labels.engageli_role }}: AWS
        Node PPS limit for local services is exceeded'
    expr: |
      label_move(
        rate(node_ethtool_linklocal_allowance_exceeded{}[2m]) > 0,
        "engageli_instance", "instance"
      )
    for: 2m
    labels:
      service: node-ec2
      severity: warning
- concurrency: 10
  interval: 60s
  name: vm-health.alerts
  rules:
  - alert: TooManyRestarts
    annotations:
      description: Job {{`{{ $labels.job }}`}} (instance {{`{{ $labels.instance }}`}})
        has restarted more than twice in the last 15 minutes. It might be crashlooping.
      summary: '{{`{{ $labels.job }}`}} too many restarts (instance {{`{{ $labels.instance
        }}`}})'
    expr: changes(process_start_time_seconds{job=~"victoria-metrics.*"}[15m]) > 2
    labels:
      service: victoria-metrics
      severity: critical
  - alert: ServiceDown
    annotations:
      description: '{{`{{ $labels.instance }}`}} of job {{`{{ $labels.job }}`}} has
        been down for more than 2 minutes.'
      summary: Service {{`{{ $labels.job }}`}} is down on {{`{{ $labels.instance }}`}}
    expr: up{job=~"victoria-metrics.*"} == 0
    for: 2m
    labels:
      service: victoria-metrics
      severity: critical
  - alert: ProcessNearFDLimits
    annotations:
      description: Exhausting OS file descriptors limit can cause severe degradation
        of the process. Consider to increase the limit as fast as possible.
      summary: Number of free file descriptors is less than 100 for "{{`{{ $labels.job
        }}`}}"("{{`{{ $labels.instance }}`}}") for the last 5m
    expr: (process_max_fds - process_open_fds) < 100
    for: 5m
    labels:
      severity: critical
  - alert: TooHighMemoryUsage
    annotations:
      description: Too high memory usage may result into multiple issues such as OOMs
        or degraded performance. Consider to either increase available memory or decrease
        the load on the process.
      summary: It is more than 80% of memory used by "{{`{{ $labels.job }}`}}"("{{`{{
        $labels.instance }}`}}") during the last 5m
    expr: (process_resident_memory_anon_bytes / vm_available_memory_bytes) > 0.8
    for: 5m
    labels:
      service: victoria-metrics
      severity: critical
  - alert: TooHighCPUUsage
    annotations:
      description: Too high CPU usage may be a sign of insufficient resources and
        make process unstable. Consider to either increase available CPU resources
        or decrease the load on the process.
      summary: More than 90% of CPU is used by "{{`{{ $labels.job }}`}}"("{{`{{ $labels.instance
        }}`}}") during the last 5m
    expr: rate(process_cpu_seconds_total[5m]) / process_cpu_cores_available > 0.9
    for: 5m
    labels:
      severity: critical
  - alert: TooManyLogs
    annotations:
      description: |-
        Logging rate for job "{{`{{ $labels.job }}`}}" ({{`{{ $labels.instance }}`}}) is {{`{{ $value }}`}} for last 15m.
         Worth to check logs for specific error messages.
      summary: Too many logs printed for job "{{`{{ $labels.job }}`}}" ({{`{{ $labels.instance
        }}`}})
    expr: sum(increase(vm_log_messages_total{level="error"}[5m])) by (job, instance)
      > 0
    for: 15m
    labels:
      service: victoria-metrics
      severity: warning
  - alert: TooManyTSIDMisses
    annotations:
      description: |-
        The rate of TSID misses during query lookups is too high for "{{`{{ $labels.job }}`}}" ({{`{{ $labels.instance }}`}}).
         Make sure you're running VictoriaMetrics of v1.85.3 or higher.
         Related issue https://github.com/VictoriaMetrics/VictoriaMetrics/issues/3502
      summary: Too many TSID misses for job "{{`{{ $labels.job }}`}}" ({{`{{ $labels.instance
        }}`}})
    expr: sum(rate(vm_missing_tsids_for_metric_id_total[5m])) by (job, instance) >
      0
    for: 10m
    labels:
      service: victoria-metrics
      severity: critical
- concurrency: 10
  interval: 60s
  name: vmsingle.alerts
  rules:
  - alert: DiskRunsOutOfSpaceIn3Days
    annotations:
      dashboard: https://engageli.grafana.net/d/wNf0q_kZk/victoriametrics?orgId=1&var-ds={{`{{
        $labels.engaeli_stack_name }}`}}-stack-prom&viewPanel=73&var-instance={{`{{
        $labels.instance }}`}}
      description: |-
        Taking into account current ingestion rate, free disk space will be enough only for {{`{{ $value | humanizeDuration }}`}} on instance {{`{{ $labels.instance }}`}}.
         Consider to limit the ingestion rate, decrease retention or scale the disk space if possible.
      summary: Instance {{`{{ $labels.instance }}`}} will run out of disk space soon
    expr: |
      vm_free_disk_space_bytes / ignoring(path)
      (
         (
          rate(vm_rows_added_to_storage_total[1d]) -
          ignoring(type) rate(vm_deduplicated_samples_total{type="merge"}[1d])
         )
        * scalar(
          sum(vm_data_size_bytes{type!~"indexdb.*"}) /
          sum(vm_rows{type!~"indexdb.*"})
         )
      ) < 3 * 24 * 3600 > 0
    for: 30m
    labels:
      service: victoria-metrics-server-single
      severity: critical
  - alert: DiskRunsOutOfSpace
    annotations:
      dashboard: https://engageli.grafana.net/d/wNf0q_kZk/victoriametrics?orgId=1&var-ds={{`{{
        $labels.engaeli_stack_name }}`}}-stack-prom&viewPanel=53&var-instance={{`{{
        $labels.instance }}`}}
      description: |-
        Disk utilisation on instance {{`{{ $labels.instance }}`}} is more than 80%.
         Having less than 20% of free disk space could cripple merges processes and overall performance. Consider to limit the ingestion rate, decrease retention or scale the disk space if possible.
      summary: Instance {{`{{ $labels.instance }}`}} will run out of disk space soon
    expr: |
      sum(vm_data_size_bytes) by(instance) /
      (
       sum(vm_free_disk_space_bytes) by(instance) +
       sum(vm_data_size_bytes) by(instance)
      ) > 0.8
    for: 30m
    labels:
      service: victoria-metrics-server-single
      severity: critical
  - alert: RequestErrorsToAPI
    annotations:
      dashboard: https://engageli.grafana.net/d/wNf0q_kZk/victoriametrics?orgId=1&var-ds={{`{{
        $labels.engaeli_stack_name }}`}}-stack-prom&viewPanel=35&var-instance={{`{{
        $labels.instance }}`}}
      description: Requests to path {{`{{ $labels.path }}`}} are receiving errors.
        Please verify if clients are sending correct requests.
      summary: Too many errors served for path {{`{{ $labels.path }}`}} (instance
        {{`{{ $labels.instance }}`}})
    expr: increase(vm_http_request_errors_total[5m]) > 0
    for: 15m
    labels:
      severity: warning
  - alert: ConcurrentFlushesHitTheLimit
    annotations:
      dashboard: https://engageli.grafana.net/d/wNf0q_kZk/victoriametrics?orgId=1&var-ds={{`{{
        $labels.engaeli_stack_name }}`}}-stack-prom&viewPanel=59&var-instance={{`{{
        $labels.instance }}`}}
      description: |-
        The limit of concurrent flushes on instance {{`{{ $labels.instance }}`}} is equal to number of CPUs.
         When VictoriaMetrics constantly hits the limit it means that storage is overloaded and requires more CPU.
      summary: VictoriaMetrics on instance {{`{{ $labels.instance }}`}} is constantly
        hitting concurrent flushes limit
    expr: avg_over_time(vm_concurrent_insert_current[1m]) >= vm_concurrent_insert_capacity
    for: 15m
    labels:
      service: victoria-metrics-server-single
      severity: warning
      show_at: dashboard
  - alert: RowsRejectedOnIngestion
    annotations:
      dashboard: https://engageli.grafana.net/d/wNf0q_kZk/victoriametrics?orgId=1&var-ds={{`{{
        $labels.engaeli_stack_name }}`}}-stack-prom&viewPanel=58&var-instance={{`{{
        $labels.instance }}`}}
      description: 'VM is rejecting to ingest rows on "{{`{{ $labels.instance }}`}}"
        due to the following reason: "{{`{{ $labels.reason }}`}}"'
      summary: Some rows are rejected on "{{`{{ $labels.instance }}`}}" on ingestion
        attempt
    expr: sum(rate(vm_rows_ignored_total[5m])) by (instance, reason) > 0
    for: 15m
    labels:
      service: victoria-metrics-server-single
      severity: warning
  - alert: TooHighChurnRate
    annotations:
      dashboard: https://engageli.grafana.net/d/wNf0q_kZk/victoriametrics?orgId=1&var-ds={{`{{
        $labels.engaeli_stack_name }}`}}-stack-prom&viewPanel=66&var-instance={{`{{
        $labels.instance }}`}}
      description: |-
        VM constantly creates new time series on "{{`{{ $labels.instance }}`}}".
         This effect is known as Churn Rate.
         High Churn Rate tightly connected with database performance and may result in unexpected OOM's or slow queries.
      summary: Churn rate is more than 10% on "{{`{{ $labels.instance }}`}}" for the
        last 15m
    expr: |
      (
         sum(rate(vm_new_timeseries_created_total[5m])) by(instance)
         /
         sum(rate(vm_rows_inserted_total[5m])) by (instance)
       ) > 0.1
    for: 15m
    labels:
      service: victoria-metrics-server-single
      severity: warning
  - alert: TooHighChurnRate24h
    annotations:
      dashboard: https://engageli.grafana.net/d/wNf0q_kZk/victoriametrics?orgId=1&var-ds={{`{{
        $labels.engaeli_stack_name }}`}}-stack-prom&viewPanel=66&var-instance={{`{{
        $labels.instance }}`}}
      description: |-
        The number of created new time series over last 24h is 3x times higher than current number of active series on "{{`{{ $labels.instance }}`}}".
         This effect is known as Churn Rate.
         High Churn Rate tightly connected with database performance and may result in unexpected OOM's or slow queries.
      summary: Too high number of new series on "{{`{{ $labels.instance }}`}}" created
        over last 24h
    expr: |
      sum(increase(vm_new_timeseries_created_total[24h])) by(instance)
      >
      (sum(vm_cache_entries{type="storage/hour_metric_ids"}) by(instance) * 3)
    for: 15m
    labels:
      service: victoria-metrics-server-single
      severity: warning
  - alert: TooHighSlowInsertsRate
    annotations:
      dashboard: https://engageli.grafana.net/d/wNf0q_kZk/victoriametrics?orgId=1&var-ds={{`{{
        $labels.engaeli_stack_name }}`}}-stack-prom&viewPanel=68&var-instance={{`{{
        $labels.instance }}`}}
      description: High rate of slow inserts on "{{`{{ $labels.instance }}`}}" may
        be a sign of resource exhaustion for the current load. It is likely more RAM
        is needed for optimal handling of the current number of active time series.
        See also https://github.com/VictoriaMetrics/VictoriaMetrics/issues/3976#issuecomment-1476883183
      summary: Percentage of slow inserts is more than 5% on "{{`{{ $labels.instance
        }}`}}" for the last 15m
    expr: |
      (
         sum(rate(vm_slow_row_inserts_total[5m])) by(instance)
         /
         sum(rate(vm_rows_inserted_total[5m])) by (instance)
       ) > 0.05
    for: 15m
    labels:
      service: victoria-metrics-server-single
      severity: warning
  - alert: LabelsLimitExceededOnIngestion
    annotations:
      dashboard: https://engageli.grafana.net/d/wNf0q_kZk/victoriametrics?orgId=1&var-ds={{`{{
        $labels.engaeli_stack_name }}`}}-stack-prom&viewPanel=74&var-instance={{`{{
        $labels.instance }}`}}
      description: |-
        VictoriaMetrics limits the number of labels per each metric with `-maxLabelsPerTimeseries` command-line flag.
         This prevents from ingesting metrics with too many labels. Please verify that `-maxLabelsPerTimeseries` is configured correctly or that clients which send these metrics aren't misbehaving.
      summary: Metrics ingested in ({{`{{ $labels.instance }}`}}) are exceeding labels
        limit
    expr: sum(increase(vm_metrics_with_dropped_labels_total[5m])) by (instance) >
      0
    for: 15m
    labels:
      service: victoria-metrics-server-single
      severity: warning
- concurrency: 10
  interval: 60s
  name: vmalert.alerts
  rules:
  - alert: ConfigurationReloadFailure
    annotations:
      description: Configuration hot-reload failed for vmalert on instance {{`{{ $labels.instance
        }}`}}. Check vmalert's logs for detailed error message.
      summary: Configuration reload failed for vmalert instance {{`{{ $labels.instance
        }}`}}
    expr: vmalert_config_last_reload_successful != 1
    labels:
      service: victoria-metrics-alerts
      severity: warning
  - alert: AlertingRulesError
    annotations:
      dashboard: https://engageli.grafana.net/d/LzldHAVnz/victoriametrics-vmalert?orgId=1&var-ds={{`{{
        $labels.engaeli_stack_name }}`}}-stack-prom&viewPanel=13&var-instance={{`{{
        $labels.instance }}`}}&var-group={{`{{ $labels.group }}`}}
      description: Alerting rules execution is failing for group "{{`{{ $labels.group
        }}`}}". Check vmalert's logs for detailed error message.
      summary: Alerting rules are failing for vmalert instance {{`{{ $labels.instance
        }}`}}
    expr: sum(vmalert_alerting_rules_error) by(job, instance, group) > 0
    for: 5m
    labels:
      service: victoria-metrics-alerts
      severity: warning
  - alert: RecordingRulesError
    annotations:
      dashboard: https://engageli.grafana.net/d/LzldHAVnz/victoriametrics-vmalert?orgId=1&var-ds={{`{{
        $labels.engaeli_stack_name }}`}}-stack-prom&viewPanel=30&var-instance={{`{{
        $labels.instance }}`}}&var-group={{`{{ $labels.group }}`}}
      description: Recording rules execution is failing for group "{{`{{ $labels.group
        }}`}}". Check vmalert's logs for detailed error message.
      summary: Recording rules are failing for vmalert instance {{`{{ $labels.instance
        }}`}}
    expr: sum(vmalert_recording_rules_error) by(job, instance, group) > 0
    for: 5m
    labels:
      service: victoria-metrics-alerts
      severity: warning
  - alert: RecordingRulesNoData
    annotations:
      dashboard: https://engageli.grafana.net/d/LzldHAVnz/victoriametrics-vmalert?orgId=1&var-ds={{`{{
        $labels.engaeli_stack_name }}`}}-stack-prom&viewPanel=33&var-group={{`{{ $labels.group
        }}`}}
      description: Recording rule "{{`{{ $labels.recording }}`}}" from group "{{`{{
        $labels.group }}`}}" produces 0 samples over the last 30min. It might be caused
        by a misconfiguration or incorrect query expression.
      summary: Recording rule {{`{{ $labels.recording }}`}} ({ $labels.group }}`}})
        produces no data
    expr: sum(vmalert_recording_rules_last_evaluation_samples) by(job, group, recording)
      < 1
    for: 30m
    labels:
      service: victoria-metrics-alerts
      severity: info
  - alert: RemoteWriteErrors
    annotations:
      description: vmalert instance {{`{{ $labels.instance }}`}} is failing to push
        metrics generated via alerting or recording rules to the configured remote
        write URL. Check vmalert's logs for detailed error message.
      summary: vmalert instance {{`{{ $labels.instance }}`}} is failing to push metrics
        to remote write URL
    expr: sum(increase(vmalert_remotewrite_errors_total[5m])) by(job, instance) >
      0
    for: 15m
    labels:
      service: victoria-metrics-alerts
      severity: warning
  - alert: AlertmanagerErrors
    annotations:
      description: vmalert instance {{`{{ $labels.instance }}`}} is failing to send
        alert notifications to "{{`{{ $labels.addr }}`}}". Check vmalert's logs for
        detailed error message.
      summary: vmalert instance {{`{{ $labels.instance }}`}} is failing to send notifications
        to Alertmanager
    expr: sum(increase(vmalert_alerts_send_errors_total[5m])) by(job, instance, addr)
      > 0
    for: 15m
    labels:
      service: victoria-metrics-alerts
      severity: warning
